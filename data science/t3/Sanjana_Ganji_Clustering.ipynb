import pandas as pd
import numpy as np
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics import davies_bouldin_score, silhouette_score
import matplotlib.pyplot as plt
import seaborn as sns

# Load the customer and transaction data
customers_df = pd.read_csv('Customers.csv')  # Customer demographic data
transactions_df = pd.read_csv('Transactions.csv')  # Product purchase data

# Clean column names to remove any extra spaces
customers_df.columns = customers_df.columns.str.strip()
transactions_df.columns = transactions_df.columns.str.strip()

# Preprocessing: Clean and extract features
def preprocess_data(customers_df, transactions_df):
    # Calculate 'Tenure' from 'SignupDate'
    customers_df['SignupDate'] = pd.to_datetime(customers_df['SignupDate'])
    customers_df['Tenure'] = (pd.to_datetime('today') - customers_df['SignupDate']).dt.days / 365.25  # tenure in years
    
    # Aggregate transaction data to calculate total spending per customer
    transaction_data = transactions_df.groupby('CustomerID').agg({'TotalValue': 'sum', 'Quantity': 'sum'}).reset_index()
    transaction_data.columns = ['CustomerID', 'TotalSpend', 'TotalQuantity']
    
    # Merge customer demographic data with transaction data
    customer_data = customers_df.merge(transaction_data, on='CustomerID', how='left')
    
    # Handle missing values by filling with 0
    customer_data = customer_data.fillna(0)
    
    # Standardize the numerical features
    numerical_features = ['Tenure', 'TotalSpend', 'TotalQuantity']
    scaler = StandardScaler()
    customer_data[numerical_features] = scaler.fit_transform(customer_data[numerical_features])
    
    return customer_data

# Preprocess the data
customers_with_features = preprocess_data(customers_df, transactions_df)

# Choose the number of clusters (between 2 and 10)
n_clusters = 5  # You can experiment with different numbers of clusters

# Perform K-Means clustering
kmeans = KMeans(n_clusters=n_clusters, random_state=42)
customers_with_features['Cluster'] = kmeans.fit_predict(customers_with_features[['Tenure', 'TotalSpend', 'TotalQuantity']])

# Calculate clustering metrics
db_index = davies_bouldin_score(customers_with_features[['Tenure', 'TotalSpend', 'TotalQuantity']], customers_with_features['Cluster'])
silhouette = silhouette_score(customers_with_features[['Tenure', 'TotalSpend', 'TotalQuantity']], customers_with_features['Cluster'])

# Inertia (Within-cluster sum of squares)
inertia = kmeans.inertia_

print(f"Number of Clusters: {n_clusters}")
print(f"Davies-Bouldin Index: {db_index}")
print(f"Silhouette Score: {silhouette}")
print(f"Inertia: {inertia}")

# Visualize the clusters using PCA for dimensionality reduction (2D visualization)
pca = PCA(n_components=2)
principal_components = pca.fit_transform(customers_with_features[['Tenure', 'TotalSpend', 'TotalQuantity']])
customers_with_features['PCA1'] = principal_components[:, 0]
customers_with_features['PCA2'] = principal_components[:, 1]

# Plot the clusters
plt.figure(figsize=(10, 8))
sns.scatterplot(data=customers_with_features, x='PCA1', y='PCA2', hue='Cluster', palette='Set1', s=100, alpha=0.6, edgecolor='w', marker='o')
plt.title(f"Customer Segments (K-Means, {n_clusters} Clusters)", fontsize=16)
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.legend(title='Cluster', loc='best')
plt.show()
